{
  "hash": "fa719328830e62bb3a0f818b120f73dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercice 2.02\"\nauthor: \"Clément Poupelin\"\ndate: \"2025-04-11\"\ndate-modified: \"2025-04-13\"\nformat: \n  html:\n    embed-resources: false\n    toc: true\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    toc-location: right\n    page-layout: article\n    code-overflow: wrap\ntoc: true\nnumber-sections: false\neditor: visual\ncategories: [\"Feuille 2\", \"Régression orthogonale\"]\nimage: \"/img/analyse-de-regression.png\"\ndescription: \"Présentation de la regression orthogonale\"\n---\n\n\n\n# Intervenant.e.s\n\n### Rédaction\n\n-   **Clément Poupelin**, [clementjc.poupelin\\@gmail.com](mailto:clementjc.poupelin@gmail.com){.email}\\\n\n### Relecture\n\n-   \n\n# Setup\n\n:::: panel-tabset\n## Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)        # manipulation des données\n```\n:::\n\n\n\n## Fonctions\n\n::: panel-tabset\n\n### Coefficients MCO\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1_MCO <- function(x, y) {\n  sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n}\n\nbeta0_MCO <- function(x, y) {\n  mean(y) - beta1_MCO(x, y) * mean(x)\n}\n```\n:::\n\n\n\n### Coefficients RI\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1_RI <- function(x, y) {\n  sum((y - mean(y))^2) / sum((x - mean(x)) * (y - mean(y)))\n}\n\nbeta0_RI <- function(x, y) {\n  mean(y) - beta1_RI(x, y) * mean(x)\n}\n```\n:::\n\n\n\n\n### Coefficients RO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1_RO <- function(x, y) {\n  delta <- (sum((y - mean(y))^2) - sum((x - mean(x))^2))^2 + 4*sum((x - mean(x)) * (y - mean(y)))^2\n  ((sum((y - mean(y))^2) - sum((x - mean(x))^2)) + sign(sum((x - mean(x)) * (y - mean(y)))) * sqrt(delta))/ ( 2 * sum((x - mean(x)) * (y - mean(y))) )\n}\n\nbeta0_RO <- function(x, y) {\n  mean(y) - beta1_RO(x, y) * mean(x)\n}\n```\n:::\n\n\n\n:::\n\n## Seed\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(140400)\n```\n:::\n\n\n\n::::\n\n# Exercice\n\n\nOn admet que le problème de minimisation associé à la régression orthogonale revient à minimise :\n\n$$S = \\sum_{i=1}^{n}\\frac{(Y_i - \\beta_0 - \\beta_1X_i)^2}{1 + \\beta_1^2}$$\n\nOn note $\\hat{\\beta}_{0\\text{RO}}$ , $\\hat{\\beta}_{1\\text{RO}}$ les paramètres du modèle de régression vérifiant ce problème de minimisation.\\\n\nTout d'abord, vérifions que $S_{XY} - \\beta_1S_{XX} = \\sum_{i=1}^n X_i\\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X})\\right]$\n\n\\begin{align*}\nS_{XY} - \\beta_1S_{XX} & = \\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y}) - \\beta_1 \\sum_{i=1}^n(X_i - \\bar{X})^2 \\\\\n\n& = \\sum_{i=1}^n \\left( (X_i - \\bar{X}) \\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right] \\right) \\\\\n\n& = \\underbrace{\\sum_{i=1}^n X_i\\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right]}_{\\text{OK}} - \\underbrace{\\bar{X} \\sum_{i=1}^n \\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right]}_{\\text{=0?}}\n\n\\end{align*}\n\nOn retrouve l'expression voulue. Vérifions l'autre égalité : \n\n\n\\begin{align*}\n\\bar{X} \\sum_{i=1}^n \\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right] & = \\bar{X} \\sum_{i=1}^n \\left[ Y_i - \\beta_1X_i - \\bar{Y} + \\beta_1\\bar{X} \\right] \\\\\n\n& = \\bar{X} \\sum_{i=1}^n \\left[ \\beta_0 + \\beta_1X_i + \\varepsilon_i - \\beta_1X_i - \\bar{Y} + \\beta_1\\bar{X} \\right] \\\\\n\n\n& = \\bar{X} \\left[ \\sum_{i=1}^n  (\\beta_0 + \\varepsilon_i) - n\\bar{Y} + n\\beta_1\\bar{X} \\right] \\\\\n\n\n& = n\\bar{X}\\beta_0 + \\bar{X}\\underbrace{\\sum_{i=1}^n \\varepsilon_i}_{=0 \\text{ car } \\varepsilon_i \\text{ centrés}} - n\\bar{X}\\bar{Y} + n\\beta_1\\bar{X}^2 \\\\\n\n\n& = n\\bar{X}\\beta_0 - n\\bar{X}\\left( \\frac{1}{n}\\sum_{i=1}^n \\left( \\beta_0 + \\beta_1X_i + \\varepsilon_i \\right) \\right) + n\\beta_1\\bar{X}^2 \\\\\n\n\n& = n\\bar{X}\\beta_0 - n\\bar{X}\\beta_0 - n\\beta_1\\bar{X}^2 - \\bar{X}\\underbrace{\\sum_{i=1}^n \\varepsilon_i}_{=0} + n\\beta_1\\bar{X}^2 \\\\\n\n& = 0\n\n\n\\end{align*}\n\n\nMaintenant, calculons les dérivées partielles $\\frac{\\partial S}{\\partial \\beta_0}$ $\\frac{\\partial S}{\\partial \\beta_1}$\n\n::: panel-tabset\n\n## $\\frac{\\partial S}{\\partial \\beta_0}$\n\n\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_0} & = \\frac{\\partial}{\\hat{\\beta}_0} \\left[\\frac{1}{1+\\beta_1^2} \\sum_{i=1}^n \\left( Y_i - \\beta_0 - \\beta_1X_i \\right)^2 \\right]\\\\\n\n& = \\frac{\\partial}{\\hat{\\beta}_0} \\left[\\frac{1}{1+\\beta_1^2} \\sum_{i=1}^n \\left( \\beta_0^2 + (Y_i - \\beta_1X_i)^2 2\\beta_0(Y_i - \\beta_1X_i) \\right) \\right]\\\\\n\n&= \\frac{2n\\beta_0}{1+\\beta_1^2} + \\frac{2 \\sum_{i=1}^n (Y_i - \\beta_1X_i)}{1+\\beta_1^2} \\\\\n\n&= \\frac{-2}{1+\\beta_1^2}\\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1X_i)\n\\end{align*}\n\n## $\\frac{\\partial S}{\\partial \\beta_1}$\n\n\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_1} & = \\frac{1}{(1 + \\beta_1^2)^2} \\left[ -2\\beta_1  \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1X_i)^2 - (1 + \\beta_1^2)2\\sum_{i=1}^n (X_i(Y_i - \\beta_0 - \\beta_1X_i)) \\right] \\\\\n\n\n\\end{align*}\n\n:::\n\n\nEnsuite, soit $\\hat{\\beta}_{0\\text{RO}}$ le point critique.\n\n\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_{0\\text{RO}}} = 0 & \\Leftrightarrow  0 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_{0\\text{RO}} - \\hat{\\beta}_{1\\text{RO}}X_i) \\\\\n\n\n& \\Leftrightarrow \\hat{\\beta}_{0\\text{RO}} = \\frac{1}{n}\\sum_{i=1}^n Y_i - \\frac{1}{n}\\sum_{i=1}^n X_i \\hat{\\beta}_{1\\text{RO}} \\\\\n\n\n& \\Leftrightarrow \\hat{\\beta}_{0\\text{RO}} = \\bar{Y} - \\hat{\\beta}_{1\\text{RO}} \\bar{X} \\\\\n\n\\end{align*}\n\n\nOn retrouve le même estimateur que pour une régression des moindres carrés ou une régression inverse. On peut donc dire que l'on passe également par le point moyen du nuage de point.\n\n\nMaintenant, cherchons l'expression de $\\hat{\\beta}_{1\\text{RO}}$\n\n\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_{1\\text{RO}}} = 0 & \\Leftrightarrow  0 = - \\hat{\\beta}_{1\\text{RO}}^2 S_{XY} + \\hat{\\beta}_{1\\text{RO}}(S_{YY} - S_{XX}) + S_{XY} \\\\\n\n\\end{align*}\n\n\nOn reconnait un polynome du second degré avec $\\Delta = (S_{YY} - S_{XX})^2 + 4S_{XY}^2 \\geq 0$\\\n\nAinsi, \n$$\\hat{\\beta}_{1\\text{RO}} = \\frac{-S_{YY} + S_{XX} \\pm \\sqrt{\\Delta}}{-2S_{XY}} = \\frac{(S_{YY} - S_{XX}) + sign(S_{XY}) \\sqrt{\\Delta}}{2S_{XY}}$$\n\n\n\nMaintenant, nous allons ajuster la régression orthogonale en reprenant le schéma de simulation de l'[exercice 4 Partie 1](../posts/Exercice_01.04.qmd)\n\n\n\n\n\n\n# Conclusion\n\n# Session info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-04-13\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n dplyr   * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}