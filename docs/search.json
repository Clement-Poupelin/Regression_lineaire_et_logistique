[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Ce blog est dédié à la reprise et des exercices de Régression linéaire et logistique enseignée dans le cadre du Master 1 IS à Nantes Université pour l’année universitaire 2022-2023.\nIl s’inscrit dans une démarche pédagogique visant à proposer des analyses claires, bien structurées et reproductibles en lien avec les thématiques abordées durant le cours."
  },
  {
    "objectID": "about.html#contexte",
    "href": "about.html#contexte",
    "title": "About",
    "section": "",
    "text": "Ce blog est dédié à la reprise et des exercices de Régression linéaire et logistique enseignée dans le cadre du Master 1 IS à Nantes Université pour l’année universitaire 2022-2023.\nIl s’inscrit dans une démarche pédagogique visant à proposer des analyses claires, bien structurées et reproductibles en lien avec les thématiques abordées durant le cours."
  },
  {
    "objectID": "about.html#objectif",
    "href": "about.html#objectif",
    "title": "About",
    "section": "Objectif",
    "text": "Objectif\nL’objectif de ce blog est de :\n\nFournir des analyses complètes et rigoureuses en réponse aux exercices abordés en travaux pratiques.\nProposer des solutions détaillées et commentées pour aider à la compréhension des méthodes statistiques.\nFavoriser une approche reproductible et bien documentée en utilisant R et Quarto."
  },
  {
    "objectID": "about.html#intervenant.e.s",
    "href": "about.html#intervenant.e.s",
    "title": "About",
    "section": "Intervenant.e.s",
    "text": "Intervenant.e.s\nLes rédacteurs et relecteurs des articles sont mentionnés sur chaque document du blog. Le travail collaboratif permet d’assurer la clarté, la rigueur et la qualité des contenus proposés."
  },
  {
    "objectID": "posts/Exercice_01.03.html",
    "href": "posts/Exercice_01.03.html",
    "title": "Exercice 1.03",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesSeed\n\n\n\n\nShow the code\nlibrary(dplyr)        # manipulation des données\n\n\n\n\n\n\nShow the code\nset.seed(140400)\n\n\n\n\n\n\n\nExercice\nOn souhaite exprimer la hauteur \\(y\\) d’un arbre en fonction de son diamètre \\(x\\) à 1m30 du sol. Pour cela, on a mesuré 20 couples diamètre-hauteur et les résultats ci-dessous sont disponibles :\n\n\\(\\bar{x} = 34.9\\)\n\\(\\bar{y} = 18.34\\)\n\\(\\frac{1}{20}\\sum_{i=1}^{20}(x_i - \\bar{x})^2 = 28.29\\)\n\\(\\frac{1}{20}\\sum_{i=1}^{20}(y_i - \\bar{y})^2 = 2.85\\)\n\\(\\frac{1}{20}\\sum_{i=1}^{20}(x_i - \\bar{x})(y_i - \\bar{y}) = 6.26\\)\n\nOn note \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\) l’estimation de la droite de régression par la méthode des moindres carrés ordinaires. Ainsi\n\n\\(\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nCe qui nous permet d’effectuer les calculs suivants :\n\nCalcul \\(\\hat{\\beta_1}\\)Calcul \\(\\hat{\\beta_0}\\)\n\n\n\\(\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{20}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{20}(x_i - \\bar{x})^2} = \\frac{6.26}{28.29} \\approx 0.22\\)\n\n\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x} = 18.34 - 0.22\\times34.9 \\approx 10.66\\)\n\n\n\nMaintenant, on peut exprimer une une mesure de qualité d’ajustement des données au modèle à l’aide des statistiques élémentaires. C’est à dire que l’on va utiliser le coefficient de corrélation :\n\\[r = \\frac{\\sum_{i=1}^{20}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{20}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{20}(y_i - \\bar{y})^2}} \\approx 0.70\\]\nIci, \\(r\\), qui est une valeur dans \\([-1, 1]\\), est suffisament proche de 1 pour considérer que le modèle est bon.\n\n\nConclusion\nDans cet exercice, nous avons pu nous entraîner à la mise en œuvre d’un modèle linéaire simple. Cette approche permet de modéliser la relation entre une variable explicative et une variable à expliquer.\nDans le cadre d’une régression simple, le coefficient de corrélation \\(r\\) constitue une mesure pertinente de la qualité de l’ajustement. Plus \\(r\\) est proche de 1 (ou de -1), plus la relation linéaire entre les deux variables est forte. En particulier, un \\(r\\) proche de 1 indique une forte corrélation positive, ce qui signifie que le modèle linéaire décrit bien la tendance générale des données.\n\nToutefois, il est important de garder à l’esprit que \\(r\\) ne suffit pas à lui seul pour évaluer la qualité d’un modèle, l’analyse des résidus et d’autres indicateurs (comme \\(R^2\\), l’erreur quadratique, …) sont également nécessaires pour une évaluation complète.\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-04-13\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n dplyr   * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_01.01.html",
    "href": "posts/Exercice_01.01.html",
    "title": "Exercice 1.01",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesSeed\n\n\n\n\nShow the code\nlibrary(dplyr)        # manipulation des données\n\n\n\n\n\n\nShow the code\nset.seed(140400)\n\n\n\n\n\n\n\nExercice\nCette exercice à pour but de présenter un peu les coefficients de corrélation de Bravais-Pearson et Spearman.\n\nOn considère deux échantillons de \\(n\\) variables \\((X_1, ..., X_n)\\) et \\((Y_1, ..., Y_n)\\).\nLa corrélation linéaire empirique \\(r\\) entre les deux échantillons (corrélation de Bravais-Pearson) ce définit comme :\n\\[r = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n(X_i - \\bar{X})^2}\\sqrt{\\sum_{i=1}^n(Y_i - \\bar{Y})^2}} \\in [-1, 1]\\] Il s’agit d’une normalisation de la covariance entre \\(X\\) et \\(Y\\) afin de la ramener entre -1 et 1.\nMaintenant, on pose la fonction \\(f\\) définie par \\(f(z) = \\sum_{i=1}^{n} (X_i - zY_i)^2\\).\nOn peut remarquer tout d’abord que :\n\n\\(f(z) \\geq 0, \\quad \\forall z \\in \\mathbb{R}\\)\n\\(f(z) = \\sum_{i=1}^{n} X_i^2 - 2z\\sum_{i=1}^{n}X_iY_i + z^2\\sum_{i=1}^{n} Y_i^2\\)\n\nAinsi, on peut reconnaitre un polynome du second degré avec \\(\\Delta = 4 \\left[ (\\sum_{i=1}^{n}X_iY_i)^2 - \\sum_{i=1}^{n}X_i^2 \\sum_{i=1}^{n}Y_i^2 \\right]\\). \nPuis comme \\(f(z) \\geq 0\\), on aura que\n\\[\\begin{align*}\n\n\\Delta \\leq 0 & \\Leftrightarrow 4 \\left[ (\\sum_{i=1}^{n}X_i Y_i)^2 - \\sum_{i=1}^{n}X_i^2 \\sum_{i=1}^{n}Y_i^2 \\right] \\leq 0 \\\\\n\n& \\Leftrightarrow (\\sum_{i=1}^{n}X_i Y_i)^2 \\leq \\sum_{i=1}^{n}X_i^2 \\sum_{i=1}^{n}Y_i^2 \\\\\n\n& \\Leftrightarrow (\\sum_{i=1}^{n}X_i Y_i)^2 \\leq \\sum_{i=1}^{n}X_i^2 \\sum_{i=1}^{n}Y_i^2 \\\\\n\n& \\Leftrightarrow \\frac{(\\sum_{i=1}^{n}X_i Y_i)^2}{\\sum_{i=1}^{n}X_i^2 \\sum_{i=1}^{n}Y_i^2} \\leq 1 \\\\\n\n& \\Leftrightarrow r^2 \\leq 1 \\\\\n\n& \\Leftrightarrow -1 \\leq r \\leq 1 \\\\\n\n\\end{align*}\\]\nMaintenant, on note \\((r_1, ..., r_n)\\) les rangs des variables \\(X_i\\) et \\((s_1, ..., s_n)\\) les rangs des variables \\(Y_i\\) dans chaque échantillon. On suppose qu’il n’y a pas d’ex-aequo, de telle sorte que les rangs vont de \\(1\\) à \\(n\\).\n\nLa corrélation de Spearman \\(\\rho\\) entre les échantillons \\((X_1, ..., X_n)\\) et \\((Y_1, ..., Y_n)\\) correspond à la corrélation linéaire entre leurs rangs.\nAinsi, on peut calculer la moyenne et la variance empirique de l’échantillon \\((r_1, ..., r_n)\\) :\n\nMoyenne empiriqueVariance empirique\n\n\n\\[\\begin{align*}\n\n\\frac{1}{n} \\sum_{i=1}^{n} r_i & = \\frac{1}{n} \\sum_{i=1}^{n} i \\\\\n\n& = \\frac{1}{n} \\frac{n(n+1)}{2} \\\\\n\n& = \\frac{n+1}{2} \\\\\n\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n\n\\frac{1}{n} \\sum_{i=1}^{n} r_i^2 - \\left( \\frac{1}{n} \\sum_{i=1}^{n} r_i \\right)^2 & = \\frac{1}{n} \\sum_{i=1}^{n} i^2 - \\left( \\frac{n+1}{2} \\right)^2 \\\\\n\n& = \\frac{1}{n} \\frac{n(n+1)(2n+1)}{6} -  \\frac{(n+1)^2}{4}  \\\\\n\n& = \\frac{(n+1)(2n+1)}{6} - \\frac{(n+1)^2}{4} \\\\\n\n& = \\frac{2(n+1)(2n+1)}{12} - \\frac{3(n+1)^2}{12} \\\\\n\n& = \\frac{1}{12} \\left[(n+1)(2(n+1) - 3(n+1))\\right] \\\\\n\n& = \\frac{1}{12} \\left[(n+1)(4n + 2 - 3n - 3)\\right] \\\\\n\n& = \\frac{1}{12} (n+1)(n-1) \\\\\n\n& = \\frac{1}{12} (n^2-1) \\\\\n\n\\end{align*}\\]\n\n\n\nCela nous permet donc de déduire que\n\\[\\begin{align*}\n\n\\rho & = \\frac{\\sum_{i=1}^n(r_i - \\bar{r})(s_i - \\bar{s})}{\\sqrt{\\sum_{i=1}^n(r_i - \\bar{r})^2}\\sqrt{\\sum_{i=1}^n(s_i - \\bar{s})^2}} \\\\\n\n& =  \\frac{ \\frac{1}{n} \\sum_{i=1}^n(r_i - \\bar{r})(s_i - \\bar{s})}{\\sqrt{  \\frac{1}{n} \\sum_{i=1}^n(r_i - \\bar{r})^2}\\sqrt{  \\frac{1}{n} \\sum_{i=1}^n(s_i - \\bar{s})^2}} \\\\\n\n\n& = \\frac{ \\frac{1}{n} \\sum_{i=1}^n r_is_i - \\frac{1}{n} \\sum_{i=1}^n r_i\\bar{s} - \\frac{1}{n} \\sum_{i=1}^n \\bar{r}s_i + \\frac{1}{n} \\sum_{i=1}^n \\bar{r} \\bar{s}}{\\sqrt{  \\frac{1}{12} n^2-1}\\sqrt{ \\frac{1}{12} n^2-1}} \\\\\n\n&= \\frac{12}{n^2-1} \\frac{1}{n} \\left[ \\sum_{i=1}^n r_is_i - \\bar{s} \\sum_{i=1}^n r_i - \\bar{r} \\sum_{i=1}^n s_i + n \\bar{r} \\bar{s} \\right]\\\\\n\n&= \\frac{12}{(n^2-1)n}  \\left[ \\sum_{i=1}^n r_is_i - \\bar{s} n\\bar{r} - \\bar{r} n\\bar{s} + n \\bar{r} \\bar{s} \\right]\\\\\n\n\n&= \\frac{12}{(n^2-1)n}  \\left[ \\sum_{i=1}^n r_is_i - \\bar{s} n\\bar{r}\\right]\\\\\n\n\n&= \\frac{12}{(n^2-1)n}  \\left[ \\sum_{i=1}^n r_is_i - n\\frac{(n+1)^2}{4} \\right]\\\\\n\n&= \\frac{12\\sum_{i=1}^n r_is_i - 3n(n+1)^2}{n(n^2-1)}\\\\\n\n\\end{align*}\\]\nEnsuite, soit \\(d_i = r_i - s_i\\). Alors\n\n\\(\\sum_{i=1}^n d_i^2 = \\sum_{i=1}^n r_i^2 + \\sum_{i=1}^ns_i^2 - 2\\sum_{i=1}^nr_is_i\\)\n\\(\\quad \\quad = \\frac{2n(n+1)(2n+1)}{6} - 2\\sum_{i=1}^nr_is_i\\)\n\\(\\Leftrightarrow  6\\sum_{i=1}^n d_i^2 =  2n(n+1)(2n+1) - 12\\sum_{i=1}^nr_is_i\\)\n\\(\\Leftrightarrow  12\\sum_{i=1}^nr_is_i  =  2n(n+1)(2n+1) - 6\\sum_{i=1}^n d_i^2\\)\n\nCe qui peut nous permettre de déduire la valeur de \\(\\rho\\) en fonction de l’échantillon des différences \\(d_i\\).\n\\[\\begin{align*}\n\n\\rho & = \\frac{12\\sum_{i=1}^n r_is_i - 3n(n+1)^2}{n(n^2-1)}\\\\\n\n& = \\frac{ 2n(n+1)(2n+1) - 6\\sum_{i=1}^n d_i^2 - 3n(n+1)^2}{n(n^2-1)}\\\\\n\n& = \\frac{ (n+1)(2n(2n+1) - 3n(n+1))  - 6\\sum_{i=1}^n d_i^2}{n(n^2-1)}\\\\\n\n& = \\frac{ (n+1)(n^2-n)  - 6\\sum_{i=1}^n d_i^2}{n(n^2-1)}\\\\\n\n& = \\frac{ (n+1)(n-1)n  - 6\\sum_{i=1}^n d_i^2}{n(n^2-1)}\\\\\n\n& = \\frac{ (n^2-1)n  - 6\\sum_{i=1}^n d_i^2}{n(n^2-1)}\\\\\n\n& = 1 - \\frac{6\\sum_{i=1}^n d_i^2}{n(n^2-1)}\\\\\n\\end{align*}\\]\n\n\nConclusion\nNous avons pu définir et illustrer de manière concrète le lien entre les deux coefficients étudiés, en mettant en évidence leur complémentarité dans l’analyse statistique. Cette démarche nous a également permis de nous entraîner à la manipulation mathématique de ces coefficients, en comprenant non seulement leur interprétation, mais aussi les relations algébriques qui les unissent. Au-delà de l’aspect purement technique, cet exercice contribue à une meilleure compréhension des outils statistiques et de leur utilité dans l’analyse de données, en posant les bases nécessaires à une utilisation rigoureuse et critique dans des contextes réels.\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-04-13\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n dplyr   * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_02.03.html",
    "href": "posts/Exercice_02.03.html",
    "title": "Exercice 2.03",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nExercice\nOn considère le modèle suivant :\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\varepsilon_i\\]\nOù les variables \\(\\varepsilon_i\\) sont i.i.d, centrées, de variance \\(\\sigma^2\\) et où les régresseurs \\(X_i\\) sont supposés i.i.d. de carré intégrable. On note \\(\\mathcal{E} = (\\varepsilon_1, . . . , \\varepsilon_n)'\\), \\(\\beta = (\\beta_0, \\beta_1)'\\) et \\(\\hat{\\beta}\\) l’estimateur de \\(\\beta\\) par MCO. On suppose que les vecteurs aléatoires \\(X\\) et \\(\\mathcal{E}\\) sont indépendants.\nOn peut commencer par exprimer \\(\\hat{\\beta} - \\beta\\) en fonction des vecteurs \\(X\\) et \\(\\varepsilon\\). En effet,\n\\[\\begin{align*}\n\n\\hat{\\beta} - \\beta &= (X'X)^{-1}X'Y - \\beta \\\\\n\n&= (X'X)^{-1} X'X\\beta + (X'X)^{-1}X'\\mathcal{E} - \\beta \\\\\n\n&= (X'X)^{-1}X'\\mathcal{E} \\\\\n\n\\end{align*}\\]\nAinsi, on peut en déduire que \\(\\hat{\\beta}\\) converge presque sûrement vers \\(\\beta\\) lorsque \\(n \\longrightarrow \\infty\\).\nOn a \\((X'X) = \\begin{pmatrix}\n  n & \\sum_{i=1}^n X_i\\\\\n  \\sum_{i=1}^n X_i & \\sum_{i=1}^n X_i^2\n\\end{pmatrix} \\Leftrightarrow \\frac{1}{n}(X'X) = \\begin{pmatrix}\n  1 & \\bar{X}\\\\\n  \\bar{X} & \\frac{1}{n} \\sum_{i=1}^n X_i^2\n\\end{pmatrix}\\)\nPar la Loi forte des grands nombre (LFGN), \\(\\frac{1}{n} (X'X) \\overset{p.s}{\\longrightarrow} M\\)\nOù \\(M = \\begin{pmatrix}\n  1 & \\mathbb{E}(X)\\\\\n  \\mathbb{E}(X) & \\mathbb{E}(X^2)\n\\end{pmatrix} \\text{ et } \\text{det}(M) = \\text{Var}(X)\\)\nEt donc \\(n(X'X)^{-1} \\overset{p.s}{\\longrightarrow} M^{-1}\\).\nDe plus, par la LFGN \\(\\frac{1}{n} X'\\mathcal{E} = \\begin{pmatrix}\n  \\frac{1}{n} \\sum_{i=1}^n \\varepsilon_i \\\\\n  \\frac{1}{n} \\sum_{i=1}^n X_i\\varepsilon_i\n\\end{pmatrix} \\overset{p.s}{\\longrightarrow} \\begin{pmatrix}\n  \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}(\\varepsilon_i) \\\\\n  \\frac{1}{n} \\sum_{i=1}^n \\underbrace{\\mathbb{E}(X_i) \\mathbb{E}(\\varepsilon_i) }_{X_i \\text{ indépendant de } \\varepsilon_i}\n\\end{pmatrix} =\\begin{pmatrix}\n  0 \\\\\n  0\n\\end{pmatrix}\\)\nDonc \\(n(X'X)^{-1} X'\\mathcal{E}\\frac{1}{n} \\overset{p.s}{\\longrightarrow} M \\begin{pmatrix}\n  0 \\\\\n  0\n\\end{pmatrix} = \\begin{pmatrix}\n  0 \\\\\n  0\n\\end{pmatrix}\\)\nDonc \\(\\hat{\\beta} - \\beta \\overset{p.s}{\\longrightarrow} 0 \\Leftrightarrow \\hat{\\beta} \\overset{p.s}{\\longrightarrow} \\beta\\)\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-04-13\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n dplyr   * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_01.02.html",
    "href": "posts/Exercice_01.02.html",
    "title": "Exercice 2",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\n\n\nFonction 1Fonction 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable de données\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n1000\n800\n600\n450\n300\n200\n100\n\n\n\\(y\\)\n573\n534\n495\n451\n395\n337\n253\n\n\n\nReprésenter sur deux graphiques différents y en fonction de x et les rangs de y en fonction des rangs de x. Commenter.\n\n\nShow the code\nlibrary(ggplot2)\n\n\nWarning: le package 'ggplot2' a été compilé avec la version R 4.2.3\n\n\nShow the code\n#### Graphe de y en fonction de x \n\n# x = c(1000, 800, 600, 450, 300, 200, 100)\n# y = c(573, 534, 495, 451, 395, 337, 253)\n\n\nd &lt;- data.frame(x = c(1000, 800, 600, 450, 300, 200, 100),\n                y = c(573, 534, 495, 451, 395, 337, 253))\n\n\nggplot(d) +\n  aes(x = x, y = y) +\n  geom_point(colour = \"red\") +\n  xlab(\"x values\") +\n  ylab(\"y values\")\n\n\n\n\n\n\n\n\n\nShow the code\n#### Graphe des rangs y en fonction des rangs x\n# \n# xtilde = rank(x)\n# ytilde = rank(y)\n\nd2 &lt;- data.frame(xtilde = rank(d$x),\n                   ytilde = rank(d$y))\n\nggplot(d2) +\n  aes(x = d2[,1], y = d2[,2]) +\n  geom_point(colour = \"red\") +\n  xlab(\"Rank(x) values\") +\n  ylab(\"Rank(y) values\")\n\n\n\n\n\n\n\n\n\nÉcrire des fonction R permettant de calculer le coefficient de Bravais-Pearson et celui de Spearman. Calculer leur valeur sur les données considérées.\n\n\nShow the code\n#### Création de fct pour les différentes corrélation \n\nS = function(x, y){\n  sum((x - mean(x)) * (y - mean(y)))\n}\n\n## Correlation Bravais-Pearson\nCorBP = function(x, y){\n  S(x, y) / sqrt( S(x, x) * S(y, y) )\n  \n}\n\n## Correlation Spearman\nCorS = function(x, y){\n  CorBP(rank(x), rank(y))\n}\n\n#### On peut comparer avec la fonction cor() déjà sur r\n\nCorS(d$x, d$y) \n\n\n[1] 1\n\n\nShow the code\ncor(d$x, d$y, method=\"spearman\")\n\n\n[1] 1\n\n\nShow the code\n#&gt; CorS(x, y) \n#[1] 1\n#&gt; cor(x, y, method=\"spearman\")\n#[1] 1\n\n\nCorBP(d$x, d$y) \n\n\n[1] 0.9624807\n\n\nShow the code\ncor(d$x, d$y, method=\"pearson\")\n\n\n[1] 0.9624807\n\n\nShow the code\n#&gt; CorBP(x, y) \n#[1] 0.9624807\n#&gt; cor(x, y, method=\"pearson\")\n#[1] 0.9624807\n\n\n\n# verif ok \n\n\nCet exemple permet d’illustrer une différence entre les deux coefficients précédents, laquelle ? Commenter.\nMaintenant, au vu de l’alignement des points sur le graphes des rank, être proche de 1 semble normal\nLe coeff de Spearman va regarder la monotonie (avec le rank) entre x et y et ici c’est monotone\nOn a une relation déterministe entre x et y. On peut donc écrire y = f(x)\n\n\nAnalyse\n\n\n\n\n\n\nNote\n\n\n\nMETTRE LES REMARQUES\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMETTRE LES POINTS D’ATTENTION\n\n\n\n\n\n\nRésultats\n\n\nMETTRE LES CONCLUSIONS\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 26100)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-04-06\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n ggplot2 * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_02.02.html",
    "href": "posts/Exercice_02.02.html",
    "title": "Exercice 2.02",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\nlibrary(dplyr)        # manipulation des données\n\n\n\n\n\nCoefficients MCOCoefficients RICoefficients RO\n\n\n\n\nShow the code\nbeta1_MCO &lt;- function(x, y) {\n  sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n}\n\nbeta0_MCO &lt;- function(x, y) {\n  mean(y) - beta1_MCO(x, y) * mean(x)\n}\n\n\n\n\n\n\nShow the code\nbeta1_RI &lt;- function(x, y) {\n  sum((y - mean(y))^2) / sum((x - mean(x)) * (y - mean(y)))\n}\n\nbeta0_RI &lt;- function(x, y) {\n  mean(y) - beta1_RI(x, y) * mean(x)\n}\n\n\n\n\n\n\nShow the code\nbeta1_RO &lt;- function(x, y) {\n  delta &lt;- (sum((y - mean(y))^2) - sum((x - mean(x))^2))^2 + 4*sum((x - mean(x)) * (y - mean(y)))^2\n  ((sum((y - mean(y))^2) - sum((x - mean(x))^2)) + sign(sum((x - mean(x)) * (y - mean(y)))) * sqrt(delta))/ ( 2 * sum((x - mean(x)) * (y - mean(y))) )\n}\n\nbeta0_RO &lt;- function(x, y) {\n  mean(y) - beta1_RO(x, y) * mean(x)\n}\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(140400)\n\n\n\n\n\n\n\nExercice\nOn admet que le problème de minimisation associé à la régression orthogonale revient à minimise :\n\\[S = \\sum_{i=1}^{n}\\frac{(Y_i - \\beta_0 - \\beta_1X_i)^2}{1 + \\beta_1^2}\\]\nOn note \\(\\hat{\\beta}_{0\\text{RO}}\\) , \\(\\hat{\\beta}_{1\\text{RO}}\\) les paramètres du modèle de régression vérifiant ce problème de minimisation.\n\nTout d’abord, vérifions que \\(S_{XY} - \\beta_1S_{XX} = \\sum_{i=1}^n X_i\\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X})\\right]\\)\n\\[\\begin{align*}\nS_{XY} - \\beta_1S_{XX} & = \\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y}) - \\beta_1 \\sum_{i=1}^n(X_i - \\bar{X})^2 \\\\\n\n& = \\sum_{i=1}^n \\left( (X_i - \\bar{X}) \\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right] \\right) \\\\\n\n& = \\underbrace{\\sum_{i=1}^n X_i\\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right]}_{\\text{OK}} - \\underbrace{\\bar{X} \\sum_{i=1}^n \\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right]}_{\\text{=0?}}\n\n\\end{align*}\\]\nOn retrouve l’expression voulue. Vérifions l’autre égalité :\n\\[\\begin{align*}\n\\bar{X} \\sum_{i=1}^n \\left[ (Y_i - \\bar{Y}) - \\beta_1(X_i - \\bar{X}) \\right] & = \\bar{X} \\sum_{i=1}^n \\left[ Y_i - \\beta_1X_i - \\bar{Y} + \\beta_1\\bar{X} \\right] \\\\\n\n& = \\bar{X} \\sum_{i=1}^n \\left[ \\beta_0 + \\beta_1X_i + \\varepsilon_i - \\beta_1X_i - \\bar{Y} + \\beta_1\\bar{X} \\right] \\\\\n\n\n& = \\bar{X} \\left[ \\sum_{i=1}^n  (\\beta_0 + \\varepsilon_i) - n\\bar{Y} + n\\beta_1\\bar{X} \\right] \\\\\n\n\n& = n\\bar{X}\\beta_0 + \\bar{X}\\underbrace{\\sum_{i=1}^n \\varepsilon_i}_{=0 \\text{ car } \\varepsilon_i \\text{ centrés}} - n\\bar{X}\\bar{Y} + n\\beta_1\\bar{X}^2 \\\\\n\n\n& = n\\bar{X}\\beta_0 - n\\bar{X}\\left( \\frac{1}{n}\\sum_{i=1}^n \\left( \\beta_0 + \\beta_1X_i + \\varepsilon_i \\right) \\right) + n\\beta_1\\bar{X}^2 \\\\\n\n\n& = n\\bar{X}\\beta_0 - n\\bar{X}\\beta_0 - n\\beta_1\\bar{X}^2 - \\bar{X}\\underbrace{\\sum_{i=1}^n \\varepsilon_i}_{=0} + n\\beta_1\\bar{X}^2 \\\\\n\n& = 0\n\n\n\\end{align*}\\]\nMaintenant, calculons les dérivées partielles \\(\\frac{\\partial S}{\\partial \\beta_0}\\) \\(\\frac{\\partial S}{\\partial \\beta_1}\\)\n\n\\(\\frac{\\partial S}{\\partial \\beta_0}\\)\\(\\frac{\\partial S}{\\partial \\beta_1}\\)\n\n\n\\[\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_0} & = \\frac{\\partial}{\\hat{\\beta}_0} \\left[\\frac{1}{1+\\beta_1^2} \\sum_{i=1}^n \\left( Y_i - \\beta_0 - \\beta_1X_i \\right)^2 \\right]\\\\\n\n& = \\frac{\\partial}{\\hat{\\beta}_0} \\left[\\frac{1}{1+\\beta_1^2} \\sum_{i=1}^n \\left( \\beta_0^2 + (Y_i - \\beta_1X_i)^2 2\\beta_0(Y_i - \\beta_1X_i) \\right) \\right]\\\\\n\n&= \\frac{2n\\beta_0}{1+\\beta_1^2} + \\frac{2 \\sum_{i=1}^n (Y_i - \\beta_1X_i)}{1+\\beta_1^2} \\\\\n\n&= \\frac{-2}{1+\\beta_1^2}\\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1X_i)\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_1} & = \\frac{1}{(1 + \\beta_1^2)^2} \\left[ -2\\beta_1  \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1X_i)^2 - (1 + \\beta_1^2)2\\sum_{i=1}^n (X_i(Y_i - \\beta_0 - \\beta_1X_i)) \\right] \\\\\n\n\n\\end{align*}\\]\n\n\n\nEnsuite, soit \\(\\hat{\\beta}_{0\\text{RO}}\\) le point critique.\n\\[\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_{0\\text{RO}}} = 0 & \\Leftrightarrow  0 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_{0\\text{RO}} - \\hat{\\beta}_{1\\text{RO}}X_i) \\\\\n\n\n& \\Leftrightarrow \\hat{\\beta}_{0\\text{RO}} = \\frac{1}{n}\\sum_{i=1}^n Y_i - \\frac{1}{n}\\sum_{i=1}^n X_i \\hat{\\beta}_{1\\text{RO}} \\\\\n\n\n& \\Leftrightarrow \\hat{\\beta}_{0\\text{RO}} = \\bar{Y} - \\hat{\\beta}_{1\\text{RO}} \\bar{X} \\\\\n\n\\end{align*}\\]\nOn retrouve le même estimateur que pour une régression des moindres carrés ou une régression inverse. On peut donc dire que l’on passe également par le point moyen du nuage de point.\nMaintenant, cherchons l’expression de \\(\\hat{\\beta}_{1\\text{RO}}\\)\n\\[\\begin{align*}\n\n\\frac{\\partial S}{\\hat{\\beta}_{1\\text{RO}}} = 0 & \\Leftrightarrow  0 = - \\hat{\\beta}_{1\\text{RO}}^2 S_{XY} + \\hat{\\beta}_{1\\text{RO}}(S_{YY} - S_{XX}) + S_{XY} \\\\\n\n\\end{align*}\\]\nOn reconnait un polynome du second degré avec \\(\\Delta = (S_{YY} - S_{XX})^2 + 4S_{XY}^2 \\geq 0\\)\n\nAinsi, \\[\\hat{\\beta}_{1\\text{RO}} = \\frac{-S_{YY} + S_{XX} \\pm \\sqrt{\\Delta}}{-2S_{XY}} = \\frac{(S_{YY} - S_{XX}) + sign(S_{XY}) \\sqrt{\\Delta}}{2S_{XY}}\\]\nMaintenant, nous allons ajuster la régression orthogonale en reprenant le schéma de simulation de l’exercice 4 Partie 1\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-04-13\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n dplyr   * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_01.04.html",
    "href": "posts/Exercice_01.04.html",
    "title": "Exercice 4",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\nlibrary(dplyr)        # manipulation des données\n\n\n\n\n\nCoefficients MCOCoefficients RI\n\n\n\n\nShow the code\nbeta1_MCO &lt;- function(x, y) {\n  sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n}\n\nbeta0_MCO &lt;- function(x, y) {\n  mean(y) - beta1_MCO(x, y) * mean(x)\n}\n\n\n\n\n\n\nShow the code\nbeta1_RI &lt;- function(x, y) {\n  sum((y - mean(y))^2) / sum((x - mean(x)) * (y - mean(y)))\n}\n\nbeta0_RI &lt;- function(x, y) {\n  mean(y) - beta1_RI(x, y) * mean(x)\n}\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(140400)\n\n\n\n\n\n\n\nDonnées\nOn va illustrer par des simulations les propriétés des estimateurs des coefficients de la régression simple par la méthode MCO et par la méthode inverse.\nPour cela on fera sur deux parties disticntes :\n\nPartie 1 : iniiation à l’estimation des coefficients de régression via des fonctions construites “à la main”\nPartie 2 : utilisation de la fonction lm pour l’estimation des coefficients de régression\n\n\nPartie 1Partie 2\n\n\nOn considère deux échantillons de \\(n\\) variables \\((X_1, . . . , X_n)\\) et \\((Y_1, . . . , Y_n)\\). On suppose que les \\((X_1, . . . , X_n)\\) sont connus. On considère le modèle de régression simple \\[Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\\] où les \\(\\varepsilon_i\\) sont i.i.d de moyenne nulle, non corrélées et de variance \\(\\sigma^2\\) .\nOn va rappeler que les coeffictients MCO sont de la forme :\n\n\\(\\hat{\\beta}_{1\\text{MCO}} = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} = \\frac{S_{XY}}{S_{XX}}\\)\n\\(\\hat{\\beta}_{0\\text{MCO}} = \\bar{Y} + \\hat{\\beta}_{1\\text{MCO}}\\bar{X}\\)\n\nMaintenant, on veut récupérer la formule des coefficients de régression inverse. Pour cela, on part d’une transformation du modèle linéaire précédent :\n\\[X_i = \\frac{-\\beta_0}{\\beta_1} + \\frac{1}{\\beta_1}Y_i + \\varepsilon_i'\\]\nCe qui nous donne les coeffficients \\(\\alpha_0 = \\frac{-\\beta_0}{\\beta_1}\\) et \\(\\alpha_1 = \\frac{1}{\\beta_1}\\). Et si on procède à une estimation MCO sur ces coefficients, on obtient :\n\n\\(\\hat{\\alpha}_{1\\text{MCO}} = \\frac{S_{XY}}{S_{YY}}\\)\n\\(\\hat{\\alpha}_{0\\text{MCO}} = \\bar{X} + \\hat{\\alpha}_{1\\text{MCO}}\\bar{Y}\\)\n\nOr, on a aussi \\(\\hat{\\alpha}_0 = \\frac{-\\hat{\\beta}_0}{\\hat{\\beta}_1}\\) et \\(\\hat{\\alpha}_1 = \\frac{1}{\\hat{\\beta}_1}\\).\nCe qui nous permet donc d’arriver aux estimateurs de régression inverse\n\n\\(\\hat{\\beta}_{1\\text{RI}} = \\frac{S_{YY}}{S_{XY}}\\)\n\\(\\hat{\\beta}_{0\\text{RI}} = \\bar{Y} + \\hat{\\beta}_{1\\text{MCO}}\\bar{X}\\)\n\nMaintenant qu’on a ces formule, on va se baser sur des données présentes dans le fichier X.txt et le but sera de calculer les estimateurs précédents sur \\(B=100\\) échantiloons de \\((Y_i)_i\\) définis par \\(Y_i = 10 + 2X_i + \\varepsilon_i\\), les \\(\\varepsilon_i\\) sont gaussiennes i.i.d de moyenne nulle, non corrélées et de variance \\(\\sigma^2=1\\).\n\n\nShow the code\nX_data &lt;- readr::read_csv(\"../Data/X.txt\")\n\n\nNew names:\nRows: 100 Columns: 2\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(2): ...1, x\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nShow the code\n## Parameters\nB &lt;- 100\n\nbeta0 &lt;- 10\nbeta1 &lt;- 2\n\nX_data$epsilon &lt;- rnorm(B, 0, 1)\n\n## Model\nX_data$Y &lt;- 10 + 2*X_data$x + X_data$epsilon\n\n\n\n\nShow the code\n# on fait nos estimateur MCO inverses\n# beta1_RI(X_data$x, 10 + 2*X_data$x + rnorm(100, 0, 1))\n\n# beta0_RI(as.numeric(X_data$x), as.numeric(10 + 2*X_data$x + rnorm(100, 0, 1)))\n#&gt; beta1RI(as.numeric(data$x), as.numeric(data$Y))\n#[1] 2.500503\n#&gt; beta0RI(as.numeric(data$x), as.numeric(data$Y))\n#[1] 9.892161\n\n\n# beta1_MCO(as.numeric(X_data$x), as.numeric(X_data$Y))\n\n# beta0_MCO(as.numeric(X_data$x), as.numeric(X_data$Y))\n#&gt; beta1MCO(as.numeric(data$x), as.numeric(data$Y))\n#[1] 1.843383\n#&gt; beta0MCO(as.numeric(data$x), as.numeric(data$Y))\n#[1] 9.89286\n\n\n\n## nos esimateurs sont pas loin des beta1 et beta0\n\n\n\nestim &lt;- data.frame(\n  beta1_RI = beta1_RI(X_data$x, 10 + 2 * X_data$x + rnorm(B, 0, 1)),\n  beta0_RI = beta0_RI(as.numeric(X_data$x), as.numeric(10 + 2 * X_data$x + rnorm(100, 0, 1))),\n  beta1_MCO = beta1_MCO(as.numeric(X_data$x), as.numeric(X_data$Y)),\n  beta0_MCO = beta0_MCO(as.numeric(X_data$x), as.numeric(X_data$Y))\n) \nestim %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn est très proche des résultats recherchées\n\nTracons les boxplot des estimateurs obtenus (on tracera les vraies valeurs des paramètres).\n\n\nShow the code\n# X_data$B1RI = 0\n# X_data$B0RI = 0\n\nfor (i in 1:B){\n  X_data$B1RI[i] &lt;- beta1_RI(X_data$x, 10 + 2*X_data$x + rnorm(B, 0, 1))\n  \n  X_data$B0RI[i] &lt;- beta0_RI(as.numeric(X_data$x), as.numeric(10 + 2*X_data$x + rnorm(B, 0, 1)))\n  \n}\n\n\nWarning: Unknown or uninitialised column: `B1RI`.\n\n\nWarning: Unknown or uninitialised column: `B0RI`.\n\n\nShow the code\n# X_data$B1MCO = 0\n# X_data$B0MCO = 0\n\nfor (i in 1:B){\n  X_data$B1MCO[i] &lt;- beta1_MCO(X_data$x, 10 + 2*X_data$x + rnorm(B, 0, 1))\n  \n  X_data$B0MCO[i] &lt;- beta0_MCO(as.numeric(X_data$x), as.numeric(10 + 2*X_data$x + rnorm(B, 0, 1)))\n  \n}\n\n\nWarning: Unknown or uninitialised column: `B1MCO`.\n\n\nWarning: Unknown or uninitialised column: `B0MCO`.\n\n\nShow the code\nboxplot(X_data$B0RI, X_data$B0MCO,\n        main = \"Estimation de Beta0 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0RI\", \"B0MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE) \nabline(v = 10, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(X_data$B1RI, X_data$B1MCO,\n        main = \"Estimation de Beta1 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1RI\", \"B1MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\nabline(v = 2, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\n########### Question 6) ##############\n\nX_data$B1RI2 = 0\nX_data$B0RI2 = 0\n\nfor (i in 1:100){\n  X_data$B1RI2[i] = beta1_RI(X_data$x, 10 + 2*X_data$x + rnorm(100, 0, 16))\n  \n  X_data$B0RI2[i] = beta0_RI(as.numeric(X_data$x), as.numeric(10 + 2*X_data$x + rnorm(100, 0, 16)))\n  \n}\n\n\nX_data$B1MCO2 = 0\nX_data$B0MCO2 = 0\n\nfor (i in 1:100){\n  X_data$B1MCO2[i] = beta1_MCO(X_data$x, 10 + 2*X_data$x + rnorm(100, 0, 16))\n  \n  X_data$B0MCO2[i] = beta0_MCO(as.numeric(X_data$x), as.numeric(10 + 2*X_data$x + rnorm(100, 0, 16)))\n  \n}\n\n\nboxplot(X_data$B0RI2, X_data$B0MCO2,\n        main = \"Estimation de Beta0 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0RI\", \"B0MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(X_data$B1RI2, X_data$B1MCO2,\n        main = \"Estimation de Beta1 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1RI\", \"B1MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\n\n\n\n\n\n\n\n\n\nShow the code\n########### Question 5) Corrigé ##############\n\n\n\nB = 10000\nn = length(X_data$x); n\n\n\n[1] 100\n\n\nShow the code\nres = matrix(0, ncol=4, nrow=B)\ndim(res)\n\n\n[1] 10000     4\n\n\nShow the code\nfor (b in 1:B){\n  eps = rnorm(n)\n  y = 10 + 2 * X_data$x + eps\n  res[b,] = c(beta1_MCO(X_data$x, y), beta0_MCO(X_data$x, y), beta1_RI(X_data$x, y), beta0_RI(X_data$x, y))\n\n}\n\nboxplot(res[,c(1,3)], \n        main = 'Comparaison des pentes', \n        names = c('MCO', 'RI'))\nabline(h = 2, col = 2)\n\n\n\n\n\n\n\n\n\nCalcul du biais et de la variance.\n\n\nShow the code\n#### faire apparaitre biais et variance ####\n\n# calcul du biais \n\nbiaisbeta1MCO = mean(res[,1] - 2)\nvariancebeta1MCO = (1/B) * sum((res[,1] - 2)**2)\n\nbiaisbeta0MCO = (1/B) * sum(res[,2] - 2)\nvariancebeta0MCO = (1/B) * sum((res[,2] - 2)**2)\n\nbiaisbeta1RI = (1/B) * sum(res[,3] - 2)\nvariancebeta1RI = (1/B) * sum((res[,3] - 2)**2)\n\nbiaisbeta0RI = (1/B) * sum(res[,4] - 2)\nvariancebeta0RI = (1/B) * sum((res[,4] - 2)**2)\n\n\nbiaisbeta1MCO\n\n\n[1] -0.001515478\n\n\nShow the code\n#[1] 0.0007703197\n\nvariancebeta1MCO\n\n\n[1] 0.01000797\n\n\nShow the code\n#[1] 0.01003526\n\nbiaisbeta0MCO\n\n\n[1] 8.001214\n\n\nShow the code\n#[1] 7.999521\n\nvariancebeta0MCO\n\n\n[1] 64.02936\n\n\nShow the code\n#[1] 64.00262\n\nbiaisbeta1RI\n\n\n[1] 0.4978257\n\n\nShow the code\n#[1] 0.4995708\n\nvariancebeta1RI\n\n\n[1] 0.2584821\n\n\nShow the code\n#[1] 0.2605581\n\nbiaisbeta0RI\n\n\n[1] 7.948585\n\n\nShow the code\n#[1] 7.946902\n\nvariancebeta0RI\n\n\n[1] 63.18995\n\n\nShow the code\n#[1] 63.16354\n\n\n#####\n\n\nboxplot(res[,c(2,4)],\n        main = \"Estimation de Beta0 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0MCO\", \"B0RI\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE) \nabline(v = 10, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(res[,c(1,3)],\n        main = \"Estimation de Beta1 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1MCO\", \"B1RI\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\nabline(v = 2, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\n## On voit que c'est des estimateurs sans biais \n\n\n########### Question 6) Corrigé ##############\n\n\nres2 = matrix(0, ncol=4, nrow=B)\n\n\nfor (b in 1:B){\n  eps = rnorm(n, 0, 16)\n  y = 10 + 2 * X_data$x + eps\n  res2[b,] = c(beta1_MCO(X_data$x, y), beta0_MCO(X_data$x, y), beta1_RI(X_data$x, y), beta0_RI(X_data$x, y))\n}\n\n\nboxplot(res2[,c(2,4)],\n        main = \"Estimation de Beta0 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0MCO\", \"B0RI\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE) \nabline(v = 10, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(res2[,c(1,3)],\n        main = \"Estimation de Beta1 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1RI\", \"B1MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\nabline(v = 2, col = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse\n\n\n\n\n\n\nNote\n\n\n\nMETTRE LES REMARQUES\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMETTRE LES POINTS D’ATTENTION\n\n\n\n\n\n\nRésultats\n\n\nMETTRE LES CONCLUSIONS\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\n# sessioninfo::session_info(pkgs = \"attached\")\n\n\n\n\nShow the code\n######################### Exercice 4 ###########################################\n################################################################################\n\n######################### Partie 1 ####\n#######################################\n\n########### Question 3) ##############\n\nbeta1MCO = function( x, y ){\n  S(x,y)/S(x,x)\n}\n\nbeta0MCO = function( x, y ){\n  mean(y) - beta1MCO(x, y) * mean(x)\n}\n\n\n########### Question 4) ##############\n\nbeta1RI = function( x, y ){\n  S(y,y)/S(x,y)\n}\n\nbeta0RI = function( x, y ){\n  mean(y) - beta1RI(x, y) * mean(x)\n}\n\n\n########### Question 5) ##############\n\nlibrary(readr)\ndata &lt;- readr::read_csv(\"../Data/X.txt\")\n\n\nNew names:\nRows: 100 Columns: 2\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(2): ...1, x\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nShow the code\n# View(data)\n\nbeta1 = 2\nbeta0 = 10\n\n\n# On créer une variable epsilon que l'on complète de 100 valeurs (taille de X)\n# qui sont des observation obtenue suivant une loi gaussienne (0, 1)\ndata$epsilon &lt;- rnorm(100, 0, 1)\n\n# on construit la variable des Yi\ndata$Y = 10 + 2*data$x + data$epsilon\n\n\n# on fait nos estimateur MCO inverses\nbeta1RI(data$x, 10 + 2*data$x + rnorm(100, 0, 1))\n\n\nError in S(y, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\nbeta0RI(as.numeric(data$x), as.numeric(10 + 2*data$x + rnorm(100, 0, 1)))\n\n\nError in S(y, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\n#&gt; beta1RI(as.numeric(data$x), as.numeric(data$Y))\n#[1] 2.500503\n#&gt; beta0RI(as.numeric(data$x), as.numeric(data$Y))\n#[1] 9.892161\n\n\nbeta1MCO(as.numeric(data$x), as.numeric(data$Y))\n\n\nError in S(x, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\nbeta0MCO(as.numeric(data$x), as.numeric(data$Y))\n\n\nError in S(x, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\n#&gt; beta1MCO(as.numeric(data$x), as.numeric(data$Y))\n#[1] 1.843383\n#&gt; beta0MCO(as.numeric(data$x), as.numeric(data$Y))\n#[1] 9.89286\n\n\n\n## nos esimateurs sont pas loin des beta1 et beta0\n\n\ndata$B1RI = 0\ndata$B0RI = 0\n\nfor (i in 1:10000){\n  data$B1RI[i] = beta1RI(data$x, 10 + 2*data$x + rnorm(100, 0, 1))\n  \n  data$B0RI[i] = beta0RI(as.numeric(data$x), as.numeric(10 + 2*data$x + rnorm(100, 0, 1)))\n  \n}\n\n\nError in S(y, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\ndata$B1MCO = 0\ndata$B0MCO = 0\n\nfor (i in 1:10000){\n  data$B1MCO[i] = beta1MCO(data$x, 10 + 2*data$x + rnorm(100, 0, 1))\n  \n  data$B0MCO[i] = beta0MCO(as.numeric(data$x), as.numeric(10 + 2*data$x + rnorm(100, 0, 1)))\n  \n}\n\n\nError in S(x, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\nboxplot(data$B0RI, data$B0MCO,\n        main = \"Estimation de Beta0 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0RI\", \"B0MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE) \nabline(v = 10, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(data$B1RI, data$B1MCO,\n        main = \"Estimation de Beta1 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1RI\", \"B1MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\nabline(v = 2, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\n########### Question 6) ##############\n\ndata$B1RI2 = 0\ndata$B0RI2 = 0\n\nfor (i in 1:10000){\n  data$B1RI2[i] = beta1RI(data$x, 10 + 2*data$x + rnorm(100, 0, 16))\n  \n  data$B0RI2[i] = beta0RI(as.numeric(data$x), as.numeric(10 + 2*data$x + rnorm(100, 0, 16)))\n  \n}\n\n\nError in S(y, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\ndata$B1MCO2 = 0\ndata$B0MCO2 = 0\n\nfor (i in 1:10000){\n  data$B1MCO2[i] = beta1MCO(data$x, 10 + 2*data$x + rnorm(100, 0, 16))\n  \n  data$B0MCO2[i] = beta0MCO(as.numeric(data$x), as.numeric(10 + 2*data$x + rnorm(100, 0, 16)))\n  \n}\n\n\nError in S(x, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\nboxplot(data$B0RI2, data$B0MCO2,\n        main = \"Estimation de Beta0 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0RI\", \"B0MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(data$B1RI2, data$B1MCO2,\n        main = \"Estimation de Beta1 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1RI\", \"B1MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\n\n\n\n\n\n\n\n\n\nShow the code\n########### Question 5) Corrigé ##############\n\n\n\nB = 10000\nn = length(data$x); n\n\n\n[1] 100\n\n\nShow the code\nres = matrix(0, ncol=4, nrow=B)\ndim(res)\n\n\n[1] 10000     4\n\n\nShow the code\nfor (b in 1:B){\n  eps = rnorm(n)\n  y = 10 + 2 * data$x + eps\n  res[b,] = c(beta1MCO(data$x, y), beta0MCO(data$x, y), beta1RI(data$x, y), beta0RI(data$x, y))\n\n}\n\n\nError in S(x, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\nboxplot(res[,c(1,3)], \n        main = 'Comparaison des pentes', \n        names = c('MCO', 'RI'))\nabline(h = 2, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\n#### faire apparaitre biais et variance ####\n\n# calcul du biais \n\nbiaisbeta1MCO = mean(res[,1] - 2)\nvariancebeta1MCO = (1/B) * sum((res[,1] - 2)**2)\n\nbiaisbeta0MCO = (1/B) * sum(res[,2] - 2)\nvariancebeta0MCO = (1/B) * sum((res[,2] - 2)**2)\n\nbiaisbeta1RI = (1/B) * sum(res[,3] - 2)\nvariancebeta1RI = (1/B) * sum((res[,3] - 2)**2)\n\nbiaisbeta0RI = (1/B) * sum(res[,4] - 2)\nvariancebeta0RI = (1/B) * sum((res[,4] - 2)**2)\n\n\nbiaisbeta1MCO\n\n\n[1] -2\n\n\nShow the code\n#[1] 0.0007703197\n\nvariancebeta1MCO\n\n\n[1] 4\n\n\nShow the code\n#[1] 0.01003526\n\nbiaisbeta0MCO\n\n\n[1] -2\n\n\nShow the code\n#[1] 7.999521\n\nvariancebeta0MCO\n\n\n[1] 4\n\n\nShow the code\n#[1] 64.00262\n\nbiaisbeta1RI\n\n\n[1] -2\n\n\nShow the code\n#[1] 0.4995708\n\nvariancebeta1RI\n\n\n[1] 4\n\n\nShow the code\n#[1] 0.2605581\n\nbiaisbeta0RI\n\n\n[1] -2\n\n\nShow the code\n#[1] 7.946902\n\nvariancebeta0RI\n\n\n[1] 4\n\n\nShow the code\n#[1] 63.16354\n\n\n#####\n\n\nboxplot(res[,c(2,4)],\n        main = \"Estimation de Beta0 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0MCO\", \"B0RI\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE) \nabline(v = 10, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(res[,c(1,3)],\n        main = \"Estimation de Beta1 avec une erreur ~N(0,1)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1MCO\", \"B1RI\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\nabline(v = 2, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\n## On voit que c'est des estimateurs sans biais \n\n\n########### Question 6) Corrigé ##############\n\n\nres2 = matrix(0, ncol=4, nrow=B)\n\n\nfor (b in 1:B){\n  eps = rnorm(n, 0, 16)\n  y = 10 + 2 * data$x + eps\n  res2[b,] = c(beta1MCO(data$x, y), beta0MCO(data$x, y), beta1RI(data$x, y), beta0RI(data$x, y))\n}\n\n\nError in S(x, y): impossible de trouver la fonction \"S\"\n\n\nShow the code\nboxplot(res2[,c(2,4)],\n        main = \"Estimation de Beta0 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B0MCO\", \"B0RI\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE) \nabline(v = 10, col = 2)\n\n\n\n\n\n\n\n\n\nShow the code\nboxplot(res2[,c(1,3)],\n        main = \"Estimation de Beta1 avec une erreur ~N(0,16)\",\n        xlab = \"xlab\",\n        #ylab = \"\",\n        names = c(\"B1RI\", \"B1MCO\"),\n        col = c(\"purple\",\"pink\"),\n        border = \"brown\",\n        horizontal = TRUE,\n        notch = TRUE)\nabline(v = 2, col = 2)\n\n\n\n\n\n\n\n\n######################### Partie 2 ####\n#######################################\n\n\n########### Question 1) ##############\nlibrary(readr)\nozone &lt;- read_csv(\"1. Workspace/Master IS/M1/Régression linéaire et logistique/TD et TP/ozone.csv\")\n\n\nError: '1. Workspace/Master IS/M1/Régression linéaire et logistique/TD et TP/ozone.csv' does not exist in current working directory ('/home/clement/Documents/1_Projet/Perso/Regression_lineaire_et_logistique/posts').\n\n\nShow the code\nView(ozone)\n\n\nError: objet 'ozone' introuvable\n\n\nShow the code\nlibrary(ggplot2)\nggplot(ozone) +\n  aes(x = ozone$T12, y = ozone$O3) +\n  geom_point(colour = 'purple') +\n  geom_smooth(formula = y~x, colour=\"red\", method=\"lm\") +\n  #geom_smooth(formula = x~y, method = \"lm\") +\n  ylab(\"Concentration en ozone\") +\n  xlab(\"Température\") +\n  ggtitle(\"Nuage de points du jeu de données Ozone\")\n\n\nError: objet 'ozone' introuvable\n\n\nShow the code\nl = lm(ozone$O3~ozone$T12, data = ozone)\n\n\nError in eval(mf, parent.frame()): objet 'ozone' introuvable\n\n\nShow the code\nl$coefficients\n\n\nError: objet 'l' introuvable\n\n\nShow the code\nsummary(l)\n\n\nError: objet 'l' introuvable\n\n\nShow the code\n#l'intercept c'est beta0 estimé et \n# l'autre valeur c'est la pente beta 1\n# on a donc beta1hat 2.7010\n#on sait que beta1hatRI = Syy / Sxy alors que beta1hatMCO = Sxy/Sxx\n\nlRI = lm(ozone$T12~ozone$O3, data = ozone) # on inverse les roles X et Y\n\n\nError in eval(mf, parent.frame()): objet 'ozone' introuvable\n\n\nShow the code\nlRI$coefficients\n\n\nError: objet 'lRI' introuvable\n\n\nShow the code\nl2 = 1 / lRI$coefficients[2] \n\n\nError: objet 'lRI' introuvable\n\n\nShow the code\nl2\n\n\nError: objet 'l2' introuvable\n\n\nShow the code\n#9.67831 \n\n\nplot(ozone$O3, ozone$T12,\n     main = \"titre\",\n     sub = eq)\n\n\nError: objet 'ozone' introuvable\n\n\nShow the code\nabline(l, col=\"blue\")\n\n\nError: objet 'l' introuvable\n\n\nShow the code\nabline(lRI, col=\"red\")\n\n\nError: objet 'lRI' introuvable\n\n\nShow the code\n########### Corrigé ##############\n\n\nplot(x&lt;-ozone$T12,y&lt;-ozone$O3,pch=20,xlab=\"Température\",\n     ylab=\"Ozone\")\n\n\nError: objet 'ozone' introuvable\n\n\nShow the code\nabline(lm(y~x),col=2,lty=2,lwd=2)\n\n\nError in eval(predvars, data, env): objet 'x' introuvable\n\n\nShow the code\nb2&lt;-1/coef(lm(x~y))[2]\n\n\nError in eval(predvars, data, env): objet 'x' introuvable\n\n\nShow the code\na2&lt;-mean(y)-b2*mean(x)\n\n\nError: objet 'b2' introuvable\n\n\nShow the code\nabline(a=a2,b=b2,col=3,lty=3,lwd=2)\n\n\nError: objet 'a2' introuvable\n\n\nShow the code\npoints(mean(x), mean(y), pch=21, col=4) # affichage du point moyen\n\n\nError: objet 'x' introuvable\n\n\nShow the code\nlegend(\"topleft\",c(\"MCO\",\"RI\"),lty=2:3,col=2:3,cex=0.8)\n\n\n\n\n\n\n\n\n\nShow the code\n## les deux droites de regression passent par le point moyen"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Régression linéaire et logistique",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nExercice 1.01\n\n\n4 min\n\n\n\nFeuille 1\n\n\nCorrélation\n\n\nBravais-Pearson\n\n\nSpearman\n\n\n\nPrésentation des coefficients de Bravais-Pearson et Spearman et leur lien linéaire\n\n\n\nClément Poupelin\n\n\nApr 11, 2025\n\n\n\n\n\n4/13/25, 11:12:01 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 1.02\n\n\n4 min\n\n\n\ncategorie 1\n\n\ncotegorie 2\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n4/13/25, 11:17:47 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 1.03\n\n\n2 min\n\n\n\nFeuille 1\n\n\nRégression\n\n\nMCO\n\n\n\nRégression simple par moindres carrés ordinaires\n\n\n\nClément Poupelin\n\n\nApr 11, 2025\n\n\n\n\n\n4/13/25, 11:12:01 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 1.04\n\n\n15 min\n\n\n\nFeuille 1\n\n\nRégression\n\n\nMCO\n\n\nRI\n\n\n\nComparaison de la méthodes des moindres carrés ordinaires et la méthode inverse pour la régression simple\n\n\n\nAuthors.s\n\n\nInvalid Date\n\n\n\n\n\n4/13/25, 11:16:02 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 2.02\n\n\n4 min\n\n\n\nFeuille 2\n\n\nRégression orthogonale\n\n\n\nPrésentation de la regression orthogonale\n\n\n\nClément Poupelin\n\n\nApr 13, 2025\n\n\n\n\n\n4/13/25, 11:13:34 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 2.03\n\n\n2 min\n\n\n\nFeuille 2\n\n\nRégresseurs aléatoires\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nApr 13, 2025\n\n\n\n\n\n4/13/25, 11:13:34 PM\n\n\n\n\n\n\n\nNo matching items"
  }
]